{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FairLend Kenya - Model Training\n",
    "## Training Credit Risk Models on Original vs Synthetic Data\n",
    "\n",
    "This notebook trains credit risk models and compares their performance and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from data_processing.bias_detector import BiasDetector\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "df_original = pd.read_csv('../data/sample_data.csv')\n",
    "print(f\"Original dataset: {df_original.shape}\")\n",
    "\n",
    "# For now, we'll use the original data. In practice, you would load synthetic data here\n",
    "# df_synthetic = pd.read_csv('../data/synthetic/fair_credit_data.csv')\n",
    "\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Prepare features for modeling\"\"\"\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    categorical_cols = ['location', 'gender', 'business_type', 'education_level']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df_model.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_model[col + '_encoded'] = le.fit_transform(df_model[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Create additional features\n",
    "    if 'monthly_income' in df_model.columns and 'loan_amount' in df_model.columns:\n",
    "        df_model['loan_to_income_ratio'] = df_model['loan_amount'] / (df_model['monthly_income'] + 1)\n",
    "    \n",
    "    if 'mpesa_transaction_count' in df_model.columns:\n",
    "        df_model['mpesa_activity_score'] = np.log1p(df_model['mpesa_transaction_count'])\n",
    "    \n",
    "    if 'credit_history_months' in df_model.columns:\n",
    "        df_model['has_credit_history'] = (df_model['credit_history_months'] > 0).astype(int)\n",
    "    \n",
    "    return df_model, label_encoders\n",
    "\n",
    "df_prepared, encoders = prepare_features(df_original)\n",
    "print(f\"\\nPrepared features: {df_prepared.shape}\")\n",
    "print(f\"\\nNew columns: {[col for col in df_prepared.columns if col not in df_original.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for modeling (excluding protected attributes directly)\n",
    "feature_cols = [\n",
    "    'age',\n",
    "    'monthly_income',\n",
    "    'mpesa_transaction_count',\n",
    "    'mpesa_avg_transaction',\n",
    "    'sacco_member',\n",
    "    'existing_loans',\n",
    "    'credit_history_months',\n",
    "    'loan_amount',\n",
    "    'loan_to_income_ratio',\n",
    "    'mpesa_activity_score',\n",
    "    'has_credit_history',\n",
    "    # Encoded categorical features\n",
    "    'location_encoded',\n",
    "    'business_type_encoded',\n",
    "    'education_level_encoded'\n",
    "]\n",
    "\n",
    "# Remove any features that don't exist\n",
    "feature_cols = [col for col in feature_cols if col in df_prepared.columns]\n",
    "\n",
    "X = df_prepared[feature_cols]\n",
    "y = df_prepared['loan_approved']\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"  AUC: {results[name]['auc']:.4f}\")\n",
    "    print(f\"  F1 Score: {results[name]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1 Score': [r['f1'] for r in results.values()],\n",
    "    'AUC': [r['auc'] for r in results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "comparison_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']].plot(kind='bar', ax=ax)\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select Best Model and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1 score\n",
    "best_model_name = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Model']\n",
    "best_model_results = results[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:\n",
    "    print(f\"  {metric.upper()}: {best_model_results[metric]:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_model_results['y_pred'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Rejected', 'Approved'],\n",
    "            yticklabels=['Rejected', 'Approved'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, best_model_results['y_pred'], \n",
    "                          target_names=['Rejected', 'Approved']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances (for tree-based models)\n",
    "best_model = best_model_results['model']\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(feature_importance.head(10)['Feature'], feature_importance.head(10)['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 10 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For logistic regression\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Coefficient': best_model.coef_[0]\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (by coefficient magnitude):\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fairness Analysis of Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test set\n",
    "test_df = df_prepared.iloc[X_test.index].copy()\n",
    "test_df['model_prediction'] = best_model_results['y_pred']\n",
    "test_df['prediction_proba'] = best_model_results['y_pred_proba']\n",
    "\n",
    "# Analyze fairness by protected attributes\n",
    "print(\"=\"*60)\n",
    "print(\"FAIRNESS ANALYSIS OF MODEL PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for attr in ['gender', 'location', 'business_type']:\n",
    "    if attr in test_df.columns:\n",
    "        print(f\"\\n{attr.upper()}:\")\n",
    "        \n",
    "        # Actual vs Predicted approval rates\n",
    "        fairness_df = test_df.groupby(attr).agg({\n",
    "            'loan_approved': 'mean',\n",
    "            'model_prediction': 'mean'\n",
    "        }).round(3)\n",
    "        fairness_df.columns = ['Actual Approval Rate', 'Predicted Approval Rate']\n",
    "        \n",
    "        print(fairness_df)\n",
    "        \n",
    "        # Calculate disparate impact for predictions\n",
    "        pred_rates = test_df.groupby(attr)['model_prediction'].mean()\n",
    "        di_pred = pred_rates.min() / pred_rates.max() if pred_rates.max() > 0 else 0\n",
    "        print(f\"  Disparate Impact (Predictions): {di_pred:.3f}\")\n",
    "        print(f\"  Status: {'⚠️ BIASED' if di_pred < 0.8 else '✅ FAIR'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare actual vs predicted approval rates by gender\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gender\n",
    "gender_comparison = test_df.groupby('gender').agg({\n",
    "    'loan_approved': 'mean',\n",
    "    'model_prediction': 'mean'\n",
    "})\n",
    "\n",
    "x = np.arange(len(gender_comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, gender_comparison['loan_approved'], width, label='Actual', alpha=0.8)\n",
    "axes[0].bar(x + width/2, gender_comparison['model_prediction'], width, label='Model Prediction', alpha=0.8)\n",
    "axes[0].set_xlabel('Gender')\n",
    "axes[0].set_ylabel('Approval Rate')\n",
    "axes[0].set_title('Approval Rates by Gender: Actual vs Predicted')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(gender_comparison.index)\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Fairness Threshold')\n",
    "\n",
    "# Location (top 5)\n",
    "location_comparison = test_df.groupby('location').agg({\n",
    "    'loan_approved': 'mean',\n",
    "    'model_prediction': 'mean'\n",
    "}).head(5)\n",
    "\n",
    "x = np.arange(len(location_comparison))\n",
    "axes[1].bar(x - width/2, location_comparison['loan_approved'], width, label='Actual', alpha=0.8)\n",
    "axes[1].bar(x + width/2, location_comparison['model_prediction'], width, label='Model Prediction', alpha=0.8)\n",
    "axes[1].set_xlabel('Location')\n",
    "axes[1].set_ylabel('Approval Rate')\n",
    "axes[1].set_title('Approval Rates by Location: Actual vs Predicted')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(location_comparison.index, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = f'../models/best_credit_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_cols, '../models/feature_columns.pkl')\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(encoders, '../models/label_encoders.pkl')\n",
    "\n",
    "print(f\"\\n✅ Model saved to {model_path}\")\n",
    "print(f\"✅ Feature columns saved\")\n",
    "print(f\"✅ Label encoders saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "1. **Best Model:** Selected based on F1 score and AUC\n",
    "2. **Model Performance:** Check accuracy, precision, recall metrics\n",
    "3. **Fairness:** Analyzed disparate impact in model predictions\n",
    "\n",
    "### Next Steps:\n",
    "1. Generate synthetic fair data (see `04_synthetic_generation.ipynb`)\n",
    "2. Retrain model on synthetic data\n",
    "3. Compare fairness metrics before and after\n",
    "4. Deploy fair model in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
